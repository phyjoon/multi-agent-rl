{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will implement a DQN agent that uses the DQN network defined in [DuellingDoubleDQN.py](DuellingDoubleDQN.py) and the replay buffer defined in [prioritized replay buffer](PrioritizedReplayBuffer.py). Notice that we also need to implement Importance Sampling in order to properly use the prioritized replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "from DuellingDoubleDQN import DuellingDQN\n",
    "from PrioritizedReplayBuffer import PrioritizedReplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuellingDoubleDQNAgent():\n",
    "    \"\"\"\n",
    "    device = cpu or gpu\n",
    "    num_agents: number of agents \n",
    "    im_height: height of input image\n",
    "    im_width: width of input image\n",
    "    obs_in_channels: number of input channels of the grid image\n",
    "    conv_dim: number of channels after passing through 1st conv. layer of the DQN \n",
    "    kernel_size: kernel size of the conv. layers in the DQN\n",
    "    n_action: number of discrete actions\n",
    "    buffer_size: replay buffer size; default 2^20 ~ 10^6; For creating sum tree it is better give buffer_size in powers of 2\n",
    "    roll_out: length of roll out for n-step bootstrap\n",
    "    replay_batch_size: batch_size of replay \n",
    "    epsilon: exploration rate\n",
    "    epsilon_decay_rate: rate by which to scale down epsilon after every few steps\n",
    "    tau: parameter for soft update of the target network\n",
    "    gamma: discount factor for discouted rewards\n",
    "    update_interval: interval after which to update the network with new parameters\n",
    "    alpha: TDerror's exponent when computing priorities of saved memories\n",
    "    beta0: Initial value of the exponent for computing importance sampling weights\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device = device, num_agents=1, im_height = 464, im_width = 464, obs_in_channels=4, conv_dim = 32,  \n",
    "                 kernel_size = 6, n_actions = 5, buffer_size = 2**20, roll_out = 5, replay_batch_size = 32,\n",
    "                 lr = 1e-5, epsilon = 0.3, epsilon_decay_rate = 0.9999, tau = 1e-3, gamma = 1, update_interval = 4, \n",
    "                 alpha = 0.6, beta0 = 0.4):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.num_agents = num_agents\n",
    "        self.im_height = im_height\n",
    "        self.im_width = im_width\n",
    "        self.in_channels = obs_in_channels\n",
    "        self.conv_dim = conv_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_actions = n_actions\n",
    "        self.buffer_size = buffer_size\n",
    "        self.roll_out = roll_out\n",
    "        self.replay_batch_size = replay_batch_size\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma # we want the train to find the shortest possible path to its destination\n",
    "                           # for every time step it gets a reward of -1\n",
    "                           # it makes sense to keep gamma = 1 \n",
    "        self.alpha = alpha\n",
    "        self.beta0 = beta0\n",
    "        # discounts to be applied at each step of roll_out\n",
    "        self.discounts = torch.tensor([self.gamma**powr \n",
    "                                       for powr in range(self.roll_out)]).float().to(device)   \n",
    "        self.update_every = update_interval        \n",
    "                \n",
    "        self.local_net = []\n",
    "        self.target_net = []\n",
    "        self.optimizer = []\n",
    "        \n",
    "        for agent in range(num_agents):\n",
    "            local = DuellingDQN(obs_in_channels, conv_dim, kernel_size, n_actions)\n",
    "            target = DuellingDQN(obs_in_channels, conv_dim, kernel_size, n_actions)\n",
    "            \n",
    "            # copy the local networks parameters to the target network\n",
    "            for local_param, target_param in zip(local.parameters(), target.parameters()):\n",
    "                target_param.data.copy_(local_param.data)\n",
    "            \n",
    "            local = local.to(device)\n",
    "            self.local_net.append(local)\n",
    "            \n",
    "            target = target.to(device)\n",
    "            self.target_net.append(target)\n",
    "            \n",
    "            # set the optimizer for the local network\n",
    "            optim = torch.optim.Adam(self.local_net[-1].parameters(), lr = lr)\n",
    "            self.optimizer.append(optim)\n",
    "        \n",
    "        # loss function to compare the Q-value of the local and the target network\n",
    "        # The total loss has to be a weighted sum of the instance losses\n",
    "        # The weights are obtained throught \"importance sampling\"\n",
    "        # Thus it is better to keep reduction to be 'none' instead of the default value which\n",
    "        # will return the average of all instance losses\n",
    "        self.criterion = nn.MSELoss(reduction = 'none')\n",
    "        # We will also be computing TDerrors for updating priorities\n",
    "        self.TDErrors = nn.L1Loss(reduction = 'none')\n",
    "        \n",
    "        # steps counter to keep track of steps passed between updates\n",
    "        self.t_step = 0\n",
    "        \n",
    "        # need to fix this to store images as memories. \n",
    "        # for the time being using a dummy value for n_states\n",
    "        n_states = 264\n",
    "        self.memory = PrioritizedReplay(buffer_size, n_states, n_actions, roll_out, num_agents, alpha)\n",
    "        \n",
    "        print(\"Created {} local networks\".format(len(self.local_net)))\n",
    "        print(\"Created {} target networks\".format(len(self.target_net)))\n",
    "        print(\"Created {} optimizers\".format(len(self.optimizer)))\n",
    "        \n",
    "    def act(self, state):\n",
    "        # function to produce an action from the DQN\n",
    "        # convert states to a torch tensor and move to the device\n",
    "        # unsqueeze at index 1 to convert the state for each agent into a batch of size 1\n",
    "        state = torch.from_numpy(state).unsqueeze(0).float().to(device)\n",
    "        expected_state_shape = (1, self.in_channels, self.im_height, self.im_width)\n",
    "        assert state.shape == expected_state_shape,\\\n",
    "        \"Error: state's shape not same as expected. Expected shape {}, got {}\".format(expected_state_shape, tuple(state.shape))\n",
    "        actions_list = []\n",
    "        with torch.no_grad():\n",
    "            for idx in range(self.num_agents):\n",
    "                self.local_net[idx].eval()\n",
    "                actionQ = self.local_net[idx](state).cpu().detach().numpy()[0]\n",
    "                    \n",
    "                # choose action with epsilon-greedy policy\n",
    "                random_num = np.random.uniform()\n",
    "                if random_num > self.epsilon:\n",
    "                    action = np.argmax(actionQ)\n",
    "                else:\n",
    "                    action = np.random.randint(self.n_actions)\n",
    "                    \n",
    "                actions_list.append(action)\n",
    "                self.local_net[idx].train()\n",
    "        actions_array = np.array(actions_list)        \n",
    "        return actions_array # each entry is the action for the corresponding agent\n",
    "    \n",
    "    \n",
    "    def step(self, new_memories):\n",
    "        \n",
    "        # new memories is a list of n tuples\n",
    "        # here n = roll_out\n",
    "        # each tuple corresponds to a step in the roll_out\n",
    "        # each tuples contains: state, actions, all_rewards, next_state, done\n",
    "        # here state and next_state are given by an image of the entire network\n",
    "        # actions is the list of each agent's action in that step\n",
    "        # thus actions[0] is the action of the 0-th agent in that step\n",
    "        self.memory.add(new_memories)\n",
    "        \n",
    "        # update the networks after every self.update_every steps\n",
    "        # make sure to check that the replay_buffer has enough memories\n",
    "        self.t_step = (self.t_step+1)%self.update_every\n",
    "        if self.t_step == 0 and self.memory.__len__() > 2*self.replay_batch_size:\n",
    "            self.learn()\n",
    "            self.epsilon = max(self.epsilon_decay_rate*self.epsilon, 0.1 )\n",
    "            self.beta0 = min(self.beta0/self.epsilon_decay_rate, 1)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def learn(self):\n",
    "        # sample a batch of memories from the replay buffer\n",
    "        batch, batch_idxs, priorities = self.memory.sample(self.replay_batch_size)\n",
    "        # for a roll_out of n-steps, the batch has shape: (self.replay_batch_size, roll_out, 5)\n",
    "        # The last dimension corresponds to shape of the tuple (state, action, all_rewards, next_state, done) for each step\n",
    "        \n",
    "        #in_states = np.stack(batch[:,0,0])\n",
    "        in_states = torch.stack(list(map(lambda mem: torch.from_numpy(mem[0][0]), batch))).float().to(device)\n",
    "        expected_in_shape = (self.replay_batch_size, self.in_channels, self.im_height, self.im_width)\n",
    "        assert in_states.shape == expected_in_shape ,\\\n",
    "        \"Error: shape of in_states is not same as expected. Expected shape: {}, got {}\".format(expected_in_shape, tuple(in_states.shape))\n",
    "        #in_states = torch.from_numpy(in_states).float().to(device)\n",
    "        \n",
    "        #actions0 = np.stack(batch[:,0,1])\n",
    "        actions0 = torch.stack(list(map(lambda mem: torch.from_numpy(mem[0][1]), batch))).to(device)\n",
    "        expected_actions_shape = (self.replay_batch_size, self.num_agents)\n",
    "        assert actions0.shape == expected_actions_shape, \\\n",
    "        \"Error: shape of actions0 not same as expected. Expected shape: {}, got {}\".format(expected_actions_shape, tuple(actions0.shape))\n",
    "        #actions0 = torch.from_numpy(actions0).to(device)\n",
    "        \n",
    "        # rewards for all the steps in the roll_out for all the agents\n",
    "        #rewards = np.array(batch[:,:,2].tolist()) \n",
    "        rewards = torch.tensor(list(map(lambda mem: list(map(lambda tup: tup[2], mem)), batch))).float().to(device)\n",
    "        expected_rewards_shape = (self.replay_batch_size, self.roll_out, self.num_agents)\n",
    "        assert rewards.shape == expected_rewards_shape, \\\n",
    "        \"Error: shape of rewards not same as expected. Expected shape: {}, got {}\".format(expected_rewards_shape, tuple(rewards.shape))\n",
    "        #rewards = torch.from_numpy(rewards).float().to(device)\n",
    "        \n",
    "        #fin_states = np.stack(batch[:,-1,3])\n",
    "        fin_states = torch.stack(list(map(lambda mem: torch.from_numpy(mem[-1][3]), batch))).float().to(device)\n",
    "        expected_fin_shape = (self.replay_batch_size, self.in_channels, self.im_height, self.im_width)\n",
    "        assert fin_states.shape == expected_fin_shape ,\\\n",
    "        \"Error: shape of fin_states is not same as expected. Expected shape: {}, got {}\".format(expected_fin_shape, tuple(fin_states.shape))\n",
    "        #fin_states = torch.from_numpy(fin_states).float().to(device)\n",
    "        \n",
    "        # each agent's done for the last step of the roll_out\n",
    "        #dones = np.stack(batch[:,:,4].tolist())\n",
    "        #expected_dones_shape = (self.replay_batch_size, self.roll_out, self.num_agents)\n",
    "        #dones = np.stack(batch[:,-1,4].tolist()) \n",
    "        dones = torch.tensor(list(map(lambda mem: mem[-1][4], batch))).float().to(device)\n",
    "        expected_dones_shape = (self.replay_batch_size, self.num_agents)\n",
    "        assert dones.shape == expected_dones_shape, \\\n",
    "        \"Error: shape of dones not same as expected. Expected shape: {}, got {}\".format(expected_dones_shape, tuple(dones.shape))\n",
    "        #dones = torch.from_numpy(dones).float().to(device)\n",
    "        \n",
    "        # compute the accumalated discounted reward over the roll_out period\n",
    "        discounted_rewards = torch.matmul(self.discounts, rewards)\n",
    "        expected_discounted_rew_shape = (self.replay_batch_size, self.num_agents)\n",
    "        assert discounted_rewards.shape == expected_discounted_rew_shape, \\\n",
    "        \"Error: shape of discounted_rewards not same as expected. Expected shape: {}, got {}\".format(expected_discounted_rew_shape, tuple(discounted_rewards.shape))\n",
    "        \n",
    "        \n",
    "        for idx in range(self.num_agents):\n",
    "            with torch.no_grad():\n",
    "                # need to choose nextQ using the local network's q-values\n",
    "                self.local_net[idx].eval()\n",
    "                nextActions = torch.max(self.local_net[idx](fin_states).detach(), axis = 1).indices.view(-1,1)\n",
    "                self.local_net[idx].train()\n",
    "                self.target_net[idx].eval()\n",
    "                nextQ = self.target_net[idx](fin_states).detach().gather(1, nextActions)\n",
    "                self.target_net[idx].train() \n",
    "                nextQ*=(1-dones[:, idx].view(-1,1))\n",
    "                targetQ = discounted_rewards[:,idx].view(-1,1) + (self.gamma**self.roll_out)*nextQ\n",
    "            \n",
    "            \n",
    "            # local agent's Q-value\n",
    "            agent_actions = actions0[:, idx].view(-1,1)\n",
    "            localQ = self.local_net[idx](in_states).gather(1, agent_actions)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # TD-errors\n",
    "                TDerrors = torch.clamp(self.TDErrors(targetQ.detach(), localQ.detach()).view(-1), -1, 1)\n",
    "                expected_TDerror_shape = (self.replay_batch_size,)\n",
    "                assert TDerrors.shape == expected_TDerror_shape,\\\n",
    "                \"Error: shape of TDerrors is not same as expected. Expected shape: {}, got {}\".format(expected_TDerror_shape, TDerrors.shape)\n",
    "                TDerrors = TDerrors.tolist()\n",
    "                \n",
    "                # update priorities according to the TDerrors\n",
    "                self.memory.update_priority(TDerrors, batch_idxs)\n",
    "                \n",
    "                # compute importance sampling weights\n",
    "                priorities_sum = self.memory.priority_tree.get_value(0)\n",
    "                priorities = torch.from_numpy(priorities).float().to(device)\n",
    "                assert priorities.shape == (self.replay_batch_size, ), \\\n",
    "                \"Error: shape of priorities is not same as expected. Expected shape: {}, got {}\".format((batch_size,), priorities.shape)\n",
    "                \n",
    "                replay_probs = priorities/priorities_sum\n",
    "                imp_sampling_weights = torch.pow(self.buffer_size*replay_probs.detach(), -self.beta0)\n",
    "                max_weight = torch.max(imp_sampling_weights)\n",
    "                imp_sampling_weights = (imp_sampling_weights/max_weight).float().to(device)\n",
    "            \n",
    "            losses = self.criterion(targetQ, localQ).view(-1)\n",
    "            assert imp_sampling_weights.shape == losses.shape,\\\n",
    "            \"Error: imp_sampling_weights and losses don't have the same shape\"\n",
    "            losses = imp_sampling_weights.detach()*losses\n",
    "            assert losses.shape == (self.replay_batch_size, ), \\\n",
    "            \"Error: shape of losses is not same as expected. Expected shape: {}, got {}\".format((batch_size,), losses.shape)\n",
    "            \n",
    "            self.optimizer[idx].zero_grad()\n",
    "            loss = losses.mean()\n",
    "            loss.backward()\n",
    "            self.optimizer[idx].step()\n",
    "            \n",
    "        \n",
    "        # now apply soft-updates to the target network\n",
    "        self.soft_updates()\n",
    "            \n",
    "        return \n",
    "    \n",
    "    \n",
    "    def soft_updates(self):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx in range(self.num_agents):\n",
    "                for target_params, local_params in zip(self.target_net[idx].parameters(), \n",
    "                                                   self.local_net[idx].parameters()):\n",
    "                    updates = self.tau*local_params.data + (1.0-self.tau)*target_params.data\n",
    "                    target_params.data.copy_(updates)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook DuellingDoubleDQNAgent.ipynb to script\n",
      "[NbConvertApp] Writing 14986 bytes to DuellingDoubleDQNAgent.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script DuellingDoubleDQNAgent.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
