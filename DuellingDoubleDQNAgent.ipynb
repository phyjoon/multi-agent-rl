{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will implement a DQN agent that uses the DQN network defined in [DuellingDoubleDQN.py](DuellingDoubleDQN.py) and the replay buffer defined in [prioritized replay buffer](PrioritizedReplayBuffer.py). Notice that we also need to implement Importance Sampling in order to properly use the prioritized replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "from DuellingDoubleDQN import DuellingDQN\n",
    "from PrioritizedReplayBuffer import PrioritizedReplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuellingDoubleDQNAgent():\n",
    "    \"\"\"\n",
    "    device = cpu or gpu\n",
    "    num_agents: number of agents \n",
    "    im_height: height of input image\n",
    "    im_width: width of input image\n",
    "    obs_in_channels: number of input channels of the grid image\n",
    "    conv_dim: number of channels after passing through 1st conv. layer of the DQN \n",
    "    kernel_size: kernel size of the conv. layers in the DQN\n",
    "    n_action: number of discrete actions\n",
    "    buffer_size: replay buffer size\n",
    "    roll_out: length of roll out for n-step bootstrap\n",
    "    replay_batch_size: batch_size of replay \n",
    "    epsilon: exploration rate\n",
    "    epsilon_decay_rate: rate by which to scale down epsilon after every few steps\n",
    "    tau: parameter for soft update of the target network\n",
    "    gamma: discount factor for discouted rewards\n",
    "    update_interval: interval after which to update the network with new parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device = device, num_agents=1, im_height = 464, im_width = 464, obs_in_channels=3, conv_dim = 32,  \n",
    "                 kernel_size = 6, n_actions = 5, buffer_size = 10**6, roll_out = 4, replay_batch_size = 32,\n",
    "                 lr = 1e-4, epsilon = 0.3, epsilon_decay_rate = 0.999, tau = 1e-3, gamma = 1, update_interval = 4):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.num_agents = num_agents\n",
    "        self.im_height = im_height\n",
    "        self.im_width = im_width\n",
    "        self.in_channels = obs_in_channels\n",
    "        self.conv_dim = conv_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_actions = n_actions\n",
    "        self.buffer_size = buffer_size\n",
    "        self.roll_out = roll_out\n",
    "        self.replay_batch_size = replay_batch_size\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma # we want the train to find the shortest possible path to its destination\n",
    "                           # for every time step it gets a reward of -1\n",
    "                           # it makes sense to keep gamma = 1 \n",
    "        self.update_every = update_interval        \n",
    "                \n",
    "        self.local_net = [DuellingDoubleDQN(obs_in_channels, conv_dim, kernel_size, n_actions) for _ in range(num_agents)]\n",
    "        self.target_net = []\n",
    "        self.optimizer = []\n",
    "        \n",
    "        for agent in range(num_agents):\n",
    "            local = self.local_net[agent]\n",
    "            target = DuellingDoubleDQN(obs_in_channels, conv_dim, kernel_size, n_actions)\n",
    "            \n",
    "            # copy the local networks parameters to the target network\n",
    "            for local_param, target_param in zip(local.parameters(), target.parameters):\n",
    "                target_param.data.copy_(local_param.data)\n",
    "            \n",
    "            self.target_net.append(target)\n",
    "            \n",
    "            local = local.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            # set the optimizer for the local network\n",
    "            optim = torch.optim.Adam(local.parameters(), lr = lr)\n",
    "        \n",
    "        # loss function to compare the Q-value of the local and the target network\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # need to fix this to store images as memories. \n",
    "        # for the time being using a dummy value for n_states\n",
    "        n_states = 264\n",
    "        self.memory = PrioritizedReplay(buffer_size, n_states, n_actions, roll_out, num_agents)\n",
    "        \n",
    "        def act(self, observations):\n",
    "            # function to produce an action from the DQN\n",
    "            \n",
    "                \n",
    "        \n",
    "                \n",
    "                \n",
    "                \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flatland-rl]",
   "language": "python",
   "name": "conda-env-flatland-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
