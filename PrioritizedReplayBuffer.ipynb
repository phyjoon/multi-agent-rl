{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will implement the priority replay buffer as described in Schaul et. al.'s [paper](https://arxiv.org/abs/1511.05952), where experiences are assigned a priority based on their TD error ($\\delta$): experiences with higher $\\delta$ are given a higher priority. New experiences for which we have not yet evaluated a TD error will be assigned the maximimal priority seen so far. In the paper they described two variants of prioritized replay:\n",
    "\n",
    "- Proportional prioritization : ```p_i = delta_i + \\epsilon ```, \" where ```epsilon``` is a small positive constant that prevents the edge-case of transitions not being revisited once their error is zero \"\n",
    "\n",
    "- Rank based prioritization: ```p_i = 1/rank_i``` where the rank is assigned to an experience based on its position if the memories were sorted in order of decreasing TD error.  \n",
    "\n",
    "Schaul et. al. report that on most of the games in the Atari 2600 suite both the Proportional and the Rank based prioritization have similar performance (although, \" there are games where one of them remains close to the Double DQN baseline while the other one leads to a big boost, for example Double Dunk or Surround for the rank-based variant, and Alien, Asterix, Enduro, Phoenix or Space\n",
    "Invaders for the proportional variant. \")\n",
    "\n",
    "In this notebook, we  will work on implementing \"Proportional prioritization\" based on a sum-tree.\n",
    "\n",
    "A sum-tree is  a binary tree where the value of the parent node is equal to the value of its two children. In our case, the leaves of the tree will correspond to the indices of the the memory buffer. The values of the leaves will correspond to the priority of the experience being stored at corresponding index in the buffer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sum_tree():\n",
    "    def __init__(self, num_memories):\n",
    "        self.num_memories = num_memories\n",
    "        self.num_levels = int(np.ceil(np.log2(num_memories)+1))\n",
    "        self.num_vertices = int(2**self.num_levels-1)\n",
    "        self.tree_array = np.zeros(shape = self.num_vertices) # the 0-th entry is the root; children of i: 2i+1, 2i+2\n",
    "        \n",
    "    def get_children(self, i):\n",
    "        # get the values of the children of the i-th node\n",
    "        try:\n",
    "            return [self.tree_array[2*i+1], self.tree_array[2*i+2]]\n",
    "        except:\n",
    "            return [None, None]\n",
    "    \n",
    "    def get_parent(self, i):\n",
    "        # get the value of the parent of the i-th node\n",
    "        return self.tree_array[(i-1)//2]\n",
    "    \n",
    "    def get_value(self, i):\n",
    "        # get the value of i-th node\n",
    "        return self.tree_array[i]\n",
    "    \n",
    "    def update_val(self, i, val):\n",
    "        # update the value of the i-th node\n",
    "        # This will also require us to update the value of its parent in order to maintian the sum-tree property\n",
    "        self.tree_array[i] = val\n",
    "        if not i==0:\n",
    "            parent = (i-1)//2\n",
    "            new_parent_val = sum(self.get_children(parent))\n",
    "            self.update_val(parent, new_parent_val)        \n",
    "            \n",
    "    def get_sample_id(self, priority, current_index = 0):\n",
    "        # get a sample corresponding to the input priority by traversing the tree starting from node at current_index\n",
    "        \n",
    "        # print(priority, current_index)\n",
    "        if priority > self.get_value(current_index):\n",
    "            raise ValueError(\"priority should be less than value of current index\")\n",
    "            \n",
    "        left_c, right_c = self.get_children(current_index)\n",
    "        \n",
    "        if left_c == None:\n",
    "            # we have reached the leaf node\n",
    "            return current_index\n",
    "        \n",
    "        if priority <= left_c:\n",
    "            sample_id = self.get_sample_id(priority, 2*current_index+1)\n",
    "            return sample_id\n",
    "        else:    \n",
    "            sample_id = self.get_sample_id(priority-left_c, 2*current_index+2)\n",
    "            return sample_id\n",
    "        \n",
    "    def max_leaf_value(self):\n",
    "        # get the maximum value amongsts all the leaves\n",
    "        return max(self.tree_array[2**(self.num_levels-2)+1:])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 7 [0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "tree = sum_tree(4)\n",
    "print(tree.num_levels, tree.num_vertices, tree.tree_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 8, 10, 12]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_indices = range(3,7)\n",
    "priorities = list(map(lambda x: 2*x, leaf_indices))\n",
    "priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, val in zip(leaf_indices, priorities):\n",
    "    tree.update_val(ind, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36., 14., 22.,  6.,  8., 10., 12.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.tree_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.get_children(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_id = tree.get_sample_id(20)\n",
    "sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.max_leaf_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.,  8., 10., 12.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.tree_array[2**(tree.num_levels-2)+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_sum = tree.get_value(0)\n",
    "num_samples = 100000\n",
    "sample_priorities = (priority_sum)*np.random.random(num_samples)\n",
    "#print(sample_priorities)\n",
    "sample_ids = list(map(lambda val: tree.get_sample_id(val), sample_priorities))\n",
    "#print(sample_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample counts: Counter({6: 33040, 5: 27974, 4: 22362, 3: 16624})\n",
      "probability of samples: {6: 0.3304, 4: 0.22362, 3: 0.16624, 5: 0.27974}\n",
      "expected probability of samples: {6: 0.3333333333333333, 4: 0.2222222222222222, 3: 0.16666666666666666, 5: 0.2777777777777778}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "sample_counts = Counter(sample_ids)\n",
    "print('sample counts: {}'.format(sample_counts))\n",
    "probs = dict(map(lambda KeyValue: (KeyValue[0], KeyValue[1]/num_samples), sample_counts.items()))\n",
    "print('probability of samples: {}'.format(probs))\n",
    "expected_probs = dict(map(lambda KeyValue: (KeyValue[0], tree.get_value(KeyValue[0])/priority_sum), sample_counts.items()))\n",
    "print('expected probability of samples: {}'.format(expected_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplay():\n",
    "    def __init__(self, buffer_size, n_states, n_actions, roll_out, n_agents, alpha = 0.6, epsilon = 0.0001):\n",
    "        self.memory = [None]*buffer_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.n_agents = n_agents\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.roll_out = roll_out # roll_out = 1 corresponds to a single step\n",
    "        self.alpha = alpha # this is the exponent \\alpha in eq.1 of the Prioritized Replay paper \n",
    "        self.epsilon = epsilon # this the epsilon  that is added to priorities to avoid edge-case issues\n",
    "                               # see the discussion below eq. 1 in Prioritized Replay paper\n",
    "        \n",
    "        # length of an array containg a single memory of any one player\n",
    "        self.experience_length = 2*n_states+n_actions+roll_out+1 \n",
    "        \n",
    "        # index the in memory where the next experience is to be added\n",
    "        # runs from 0 to buffer_size-1 after which it resets to zero\n",
    "        self.new_exp_idx = 0\n",
    "        \n",
    "        # sum tree for the priorities\n",
    "        self.priority_tree = sum_tree(buffer_size)\n",
    "        self.first_leaf_idx = 2**(self.priority_tree.num_levels-1)-1 # index of the first leaf in priority_tree.tree_array \n",
    "        \n",
    "    def add(self, experience_tuple):\n",
    "        # add a new experience to the memory\n",
    "        # each tuple consists of (n-1)-steps of state, action, reward, done and the n-state\n",
    "        # here n is the roll_out length\n",
    "        self.memory[self.new_exp_idx] = experience_tuple\n",
    "        \n",
    "        # get the maximal priority of all experiences in the buffer\n",
    "        priority = max([*self.priority_tree.tree_array[self.first_leaf_idx:], self.epsilon])\n",
    "        \n",
    "        # now update the priority in the priority_tree\n",
    "        tree_index = self.new_exp_idx + self.first_leaf_idx\n",
    "        self.priority_tree.update_val(tree_index, priority)\n",
    "        \n",
    "        # move the new experience index to the next position\n",
    "        self.new_exp_idx = (self.new_exp_idx+1)%self.buffer_size\n",
    "    \n",
    "    def update_priority(self, TDErrors, experience_idxs):\n",
    "        # *********** To Implement **************\n",
    "        # update the priorities of the given experiences\n",
    "        # priority = abs(TDError)**self.alpha + self.epsilon\n",
    "        # experience_idxs are the indices of the experiences in self.memory whose priority has to be updated\n",
    "        # TDErrors are the latest TDErrors of those experiences\n",
    "        for TDError, idx in zip(TDErrors, experience_idxs):\n",
    "            priority = abs(TDError)**self.alpha + self.epsilon\n",
    "            # now update the priority in the priority_tree\n",
    "            tree_index = idx + self.first_leaf_idx\n",
    "            self.priority_tree.update_val(tree_index, priority)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        # get sum of all priorities \n",
    "        priority_sum = self.priority_tree.get_value(0)\n",
    "        sample_priorities = (priority_sum)*np.random.random(batch_size)\n",
    "        \n",
    "        \n",
    "        batch_idxs = np.array(list(map(lambda val: \n",
    "                                      self.priority_tree.get_sample_id(val) - self.first_leaf_idx, sample_priorities)))\n",
    "        \n",
    "        batch = np.stack(list(itemgetter(*batch_idxs)(self.memory)))\n",
    "        print(batch)\n",
    "        expected_batch_shape = (batch_size, self.n_agents, self.experience_length)\n",
    "        \n",
    "        assert batch.shape == expected_batch_shape, 'Shape of the batch is not same as expected. Got: {}, expected: {}!'.format(batch.shape, expected_batch_shape)\n",
    "        \n",
    "        states0_batch = batch[:,:,:self.n_states] # shape = (batch_size, n_agents, n_states)\n",
    "        actions0_batch = batch[:,:, self.n_states: self.n_states+self.n_actions].reshape(batch_size, -1)\n",
    "        # shape = (batch_size, n_agents*n_actions)\n",
    "        assert actions0_batch.shape == (batch_size, self.n_agents*self.n_actions),  'actions0 shape is incorrect'\n",
    "        \n",
    "        rewards_batch = batch[:,:,self.n_states+self.n_actions:self.n_states+self.n_actions+self.roll_out] # shape = (batch_size, n_agents, roll_out)\n",
    "        dones = batch[:,0,self.n_states+self.n_actions+self.roll_out:self.n_states+self.n_actions+self.roll_out+1]\n",
    "        # shape = (batch_size, 1)\n",
    "        states_fin_batch = batch[:,:,self.n_states+self.n_actions+self.roll_out+1:] # shape = (batch_size, n_agents, n_states)\n",
    "        \n",
    "        return  states0_batch, actions0_batch, rewards_batch, dones, states_fin_batch, batch_idxs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 2**3\n",
    "n_states = 2\n",
    "n_actions = 2\n",
    "roll_out = 1\n",
    "n_agents = 1\n",
    "replay_buffer = PrioritizedReplay(buffer_size, n_states, n_actions, roll_out, n_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = np.random.random(size = (buffer_size, n_agents, n_states + n_actions + roll_out + 1 + n_states ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp[0,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experience in exp:\n",
    "    replay_buffer.add(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0008, 0.0004, 0.0004, 0.0002, 0.0002, 0.0002, 0.0002, 0.0001,\n",
       "       0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.priority_tree.tree_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample counts: Counter({9: 12569, 13: 12564, 14: 12539, 10: 12504, 7: 12498, 8: 12495, 11: 12433, 12: 12398})\n",
      "probability of samples: {8: 0.12495, 11: 0.12433, 7: 0.12498, 12: 0.12398, 14: 0.12539, 9: 0.12569, 10: 0.12504, 13: 0.12564}\n",
      "expected probability of samples: {8: 0.125, 11: 0.125, 7: 0.125, 12: 0.125, 14: 0.125, 9: 0.125, 10: 0.125, 13: 0.125}\n"
     ]
    }
   ],
   "source": [
    "priority_sum = replay_buffer.priority_tree.get_value(0)\n",
    "num_samples = 100000\n",
    "sample_priorities = (priority_sum)*np.random.random(num_samples)\n",
    "#print(sample_priorities)\n",
    "sample_ids = list(map(lambda val: replay_buffer.priority_tree.get_sample_id(val), sample_priorities))\n",
    "#print(sample_ids)\n",
    "\n",
    "from collections import Counter\n",
    "sample_counts = Counter(sample_ids)\n",
    "print('sample counts: {}'.format(sample_counts))\n",
    "probs = dict(map(lambda KeyValue: (KeyValue[0], KeyValue[1]/num_samples), sample_counts.items()))\n",
    "print('probability of samples: {}'.format(probs))\n",
    "expected_probs = dict(map(lambda KeyValue: (KeyValue[0], \n",
    "                                            replay_buffer.priority_tree.get_value(KeyValue[0])/priority_sum), sample_counts.items()))\n",
    "print('expected probability of samples: {}'.format(expected_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.56128528 0.36002206 0.63071026 0.47259625 0.82141372 0.47588885\n",
      "   0.43940773 0.37293861]]\n",
      "\n",
      " [[0.73405148 0.11983133 0.90191481 0.67229982 0.40811169 0.11024123\n",
      "   0.75844642 0.34592707]]]\n"
     ]
    }
   ],
   "source": [
    "*mems, indxs = replay_buffer.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33908088 0.09650215]\n"
     ]
    }
   ],
   "source": [
    "tderrors = np.random.random(2)\n",
    "print(tderrors)\n",
    "replay_buffer.update_priority(tderrors, indxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.69294561e-01, 4.00000000e-04, 7.68894561e-01, 2.00000000e-04,\n",
       "       2.00000000e-04, 2.46079433e-01, 5.22815128e-01, 1.00000000e-04,\n",
       "       1.00000000e-04, 1.00000000e-04, 1.00000000e-04, 1.00000000e-04,\n",
       "       2.45979433e-01, 5.22715128e-01, 1.00000000e-04])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.priority_tree.tree_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52271513, 0.24597943])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tderrors**0.6+0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flatland-rl]",
   "language": "python",
   "name": "conda-env-flatland-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
