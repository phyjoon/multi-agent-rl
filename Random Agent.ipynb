{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent \n",
    "\n",
    "This is a random agent as implemented in examples provided [here](https://gitlab.aicrowd.com/flatland/flatland/blob/master/examples/training_example.py) by the Flatland challenge creators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from flatland.envs.observations import TreeObsForRailEnv, LocalObsForRailEnv\n",
    "from flatland.envs.predictions import ShortestPathPredictorForRailEnv\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.envs.rail_generators import complex_rail_generator\n",
    "from flatland.envs.schedule_generators import complex_schedule_generator\n",
    "from flatland.utils.rendertools import RenderTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Use the complex_rail_generator to generate feasible network configurations with corresponding tasks\n",
    "# Training on simple small tasks is the best way to get familiar with the environment\n",
    "N_agents = 1\n",
    "\n",
    "TreeObservation = TreeObsForRailEnv(max_depth=2, predictor=ShortestPathPredictorForRailEnv())\n",
    "LocalGridObs = LocalObsForRailEnv(view_height=10, view_width=2, center=2)\n",
    "rail_generator = complex_rail_generator(nr_start_goal=10, nr_extra=2, min_dist=8, max_dist=99999, seed=1)\n",
    "env = RailEnv(width=20, height=20,\n",
    "              rail_generator= rail_generator,\n",
    "              schedule_generator=complex_schedule_generator(), \n",
    "              number_of_agents=N_agents, \n",
    "              obs_builder_object=TreeObservation)\n",
    "env.reset()\n",
    "\n",
    "env_renderer = RenderTool(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your own Agent or use RLlib to train agents on Flatland\n",
    "# As an example we use a random agent here\n",
    "class RandomAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        :param state: input is the observation of the agent\n",
    "        :return: returns an action\n",
    "        \"\"\"\n",
    "        return np.random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def step(self, memories):\n",
    "        \"\"\"\n",
    "        Step function to improve agent by adjusting policy given the observations\n",
    "\n",
    "        :param memories: SARS Tuple to be\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    def save(self, filename):\n",
    "        # Store the current policy\n",
    "        return\n",
    "\n",
    "    def load(self, filename):\n",
    "        # Load a policy\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the agent with the parameters corresponding to the environment and observation_builder\n",
    "agent = RandomAgent(218, 5)\n",
    "n_trials = 20\n",
    "\n",
    "# Empty dictionary for all agent action\n",
    "action_dict = dict()\n",
    "print(\"Starting Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Nr. 20\t Score = -33.0"
     ]
    }
   ],
   "source": [
    "all_scores = []\n",
    "for trials in range(1, n_trials + 1):\n",
    "\n",
    "    # Reset environment and get initial observations for all agents\n",
    "    obs, info = env.reset()\n",
    "    for idx in range(env.get_num_agents()):\n",
    "        tmp_agent = env.agents[idx]\n",
    "        tmp_agent.speed_data[\"speed\"] = 1 / (idx + 1)\n",
    "    env_renderer.reset()\n",
    "    # Here you can also further enhance the provided observation by means of normalization\n",
    "    # See training navigation example in the baseline repository\n",
    "\n",
    "    score = 0\n",
    "    # Run episode\n",
    "    for step in range(50):\n",
    "        # Chose an action for each agent in the environment\n",
    "        for a in range(env.get_num_agents()):\n",
    "            action = agent.act(obs[a])\n",
    "            action_dict.update({a: action})\n",
    "        # Environment step which returns the observations for all agents, their corresponding\n",
    "        # reward and whether their are done\n",
    "        next_obs, all_rewards, done, _ = env.step(action_dict)\n",
    "        # note that rendering significantly slows down the average run time\n",
    "        # consider commenting the rendering command for a faster run\n",
    "        # env_renderer.render_env(show=True, show_observations=True, show_predictions=False)\n",
    "\n",
    "        # Update replay buffer and train agent\n",
    "        for a in range(env.get_num_agents()):\n",
    "            agent.step((obs[a], action_dict[a], all_rewards[a], next_obs[a], done[a]))\n",
    "            score += all_rewards[a]\n",
    "        obs = next_obs.copy()\n",
    "        if done['__all__']:\n",
    "            break\n",
    "    all_scores.append(score)        \n",
    "    print('\\rEpisode Nr. {}\\t Score = {}'.format(trials, score), end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score of a random agent over 20 episodes is -43.6\n"
     ]
    }
   ],
   "source": [
    "avg = np.mean(all_scores)\n",
    "print('average score of a random agent over {} episodes is {}'.format(n_trials, avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent with renderer without background embellishments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to check that the renderer works fine even after removing background embellishments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "#import tkinter as tk\n",
    "\n",
    "from flatland.envs.observations import TreeObsForRailEnv, LocalObsForRailEnv\n",
    "from flatland.envs.predictions import ShortestPathPredictorForRailEnv\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.envs.rail_generators import complex_rail_generator\n",
    "from flatland.envs.schedule_generators import complex_schedule_generator\n",
    "from flatland.utils.rendertools import RenderTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from numpy import array\n",
    "from pkg_resources import resource_string as resource_bytes\n",
    "\n",
    "from flatland.utils.graphics_layer import GraphicsLayer\n",
    "\n",
    "from flatland.core.grid.rail_env_grid import RailEnvTransitions \n",
    "\n",
    "from flatland.utils.graphics_pil import PILGL, PILSVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Use the complex_rail_generator to generate feasible network configurations with corresponding tasks\n",
    "# Training on simple small tasks is the best way to get familiar with the environment\n",
    "N_agents = 1\n",
    "\n",
    "TreeObservation = TreeObsForRailEnv(max_depth=2, predictor=ShortestPathPredictorForRailEnv())\n",
    "LocalGridObs = LocalObsForRailEnv(view_height=10, view_width=2, center=2)\n",
    "rail_generator = complex_rail_generator(nr_start_goal=10, nr_extra=2, min_dist=8, max_dist=99999, seed=1)\n",
    "env = RailEnv(width=20, height=20,\n",
    "              rail_generator= rail_generator,\n",
    "              schedule_generator=complex_schedule_generator(), \n",
    "              number_of_agents=N_agents, \n",
    "              obs_builder_object=TreeObservation)\n",
    "env.reset()\n",
    "\n",
    "env_renderer = RenderTool(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_rail_at_new(self, row, col, binary_trans, target=None, is_selected=False, rail_grid=None, num_agents=None,\n",
    "                show_debug=True):\n",
    "    \n",
    "    if binary_trans in self.pil_rail:\n",
    "        pil_track = self.pil_rail[binary_trans]\n",
    "        if target is not None:\n",
    "            target_img = self.station_colors[target % len(self.station_colors)]\n",
    "            target_img = Image.alpha_composite(pil_track, target_img)\n",
    "            self.draw_image_row_col(target_img, (row, col), layer=PILGL.TARGET_LAYER)\n",
    "            if show_debug:\n",
    "                self.text_rowcol((row + 0.8, col + 0.0), strText=str(target), layer=PILGL.TARGET_LAYER)\n",
    "\n",
    "        city_size = 1\n",
    "        if num_agents is not None:\n",
    "            city_size = max(1, np.log(1 + num_agents) / 2.5)\n",
    "\n",
    "        self.draw_image_row_col(pil_track, (row, col), layer=PILGL.RAIL_LAYER)\n",
    "    else:\n",
    "        print(\"Illegal rail:\", row, col, format(binary_trans, \"#018b\")[2:], binary_trans)\n",
    "\n",
    "    if target is not None:\n",
    "        if is_selected:\n",
    "            svgBG = self.pil_from_png_file('flatland.png', \"Selected_Target.png\")\n",
    "            self.clear_layer(PILGL.SELECTED_TARGET_LAYER, 0)\n",
    "            self.draw_image_row_col(svgBG, (row, col), layer=PILGL.SELECTED_TARGET_LAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcType = types.MethodType\n",
    "env_renderer = RenderTool(env)\n",
    "env_renderer.gl.set_rail_at = funcType(set_rail_at_new, env_renderer.gl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your own Agent or use RLlib to train agents on Flatland\n",
    "# As an example we use a random agent here\n",
    "class RandomAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        :param state: input is the observation of the agent\n",
    "        :return: returns an action\n",
    "        \"\"\"\n",
    "        return np.random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def step(self, memories):\n",
    "        \"\"\"\n",
    "        Step function to improve agent by adjusting policy given the observations\n",
    "\n",
    "        :param memories: SARS Tuple to be\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    def save(self, filename):\n",
    "        # Store the current policy\n",
    "        return\n",
    "\n",
    "    def load(self, filename):\n",
    "        # Load a policy\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the agent with the parameters corresponding to the environment and observation_builder\n",
    "agent = RandomAgent(218, 5)\n",
    "n_trials = 5000\n",
    "\n",
    "# Empty dictionary for all agent action\n",
    "action_dict = dict()\n",
    "print(\"Starting Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Nr. 5000\tScore = -124.0\tAverage Score: -124.00"
     ]
    }
   ],
   "source": [
    "all_scores = []\n",
    "for trials in range(1, n_trials + 1):\n",
    "\n",
    "    # Reset environment and get initial observations for all agents\n",
    "    obs, info = env.reset()\n",
    "    for idx in range(env.get_num_agents()):\n",
    "        tmp_agent = env.agents[idx]\n",
    "        tmp_agent.speed_data[\"speed\"] = 1 / (idx + 1)\n",
    "    env_renderer.reset()\n",
    "    # Here you can also further enhance the provided observation by means of normalization\n",
    "    # See training navigation example in the baseline repository\n",
    "\n",
    "    score = 0\n",
    "    scores_window = deque(maxlen = 100) # scores of last 100 episodes\n",
    "    # Run episode\n",
    "    for step in range(500):\n",
    "        # Chose an action for each agent in the environment\n",
    "        for a in range(env.get_num_agents()):\n",
    "            action = agent.act(obs[a])\n",
    "            action_dict.update({a: action})\n",
    "        # Environment step which returns the observations for all agents, their corresponding\n",
    "        # reward and whether their are done\n",
    "        next_obs, all_rewards, done, _ = env.step(action_dict)\n",
    "        # note that rendering significantly slows down the average run time\n",
    "        # consider commenting the rendering command for a faster run\n",
    "        # env_renderer.render_env(show=True, show_observations=True, show_predictions=False)\n",
    "\n",
    "        # Update replay buffer and train agent\n",
    "        for a in range(env.get_num_agents()):\n",
    "            agent.step((obs[a], action_dict[a], all_rewards[a], next_obs[a], done[a]))\n",
    "            score += all_rewards[a]\n",
    "        obs = next_obs.copy()\n",
    "        if done['__all__']:\n",
    "            break\n",
    "    all_scores.append(score)\n",
    "    scores_window.append(score)\n",
    "    print('\\rEpisode Nr. {}\\tScore = {}\\tAverage Score: {:.2f}'.format(trials, score, np.mean(scores_window)), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score of a random agent over 5000 episodes is -272.2512\n"
     ]
    }
   ],
   "source": [
    "avg = np.mean(all_scores)\n",
    "print('average score of a random agent over {} episodes is {}'.format(n_trials, avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(1,len(all_scores)+1), all_scores )\n",
    "plt.xlabel(\"Episode no.\")\n",
    "plt.ylabel(\"Episode score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the scores are uniformly distributed between 0 and 500. The expectation value of the random uniform distribution over an interval (a,b) is given by (b+a)/2. In our case we therefore expect the avg. score to be (0+500)/2 = 250.0. The avg. score we found experimentally was ~ 272 which not too far from the theoretical expectation value for the score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work fine!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flatland-rl]",
   "language": "python",
   "name": "conda-env-flatland-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
