{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent \n",
    "\n",
    "This is a random agent as implemented in examples provided [here](https://gitlab.aicrowd.com/flatland/flatland/blob/master/examples/training_example.py) by the Flatland challenge creators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from flatland.envs.observations import TreeObsForRailEnv, LocalObsForRailEnv\n",
    "from flatland.envs.predictions import ShortestPathPredictorForRailEnv\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.envs.rail_generators import complex_rail_generator\n",
    "from flatland.envs.schedule_generators import complex_schedule_generator\n",
    "from flatland.utils.rendertools import RenderTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Use the complex_rail_generator to generate feasible network configurations with corresponding tasks\n",
    "# Training on simple small tasks is the best way to get familiar with the environment\n",
    "N_agents = 1\n",
    "\n",
    "TreeObservation = TreeObsForRailEnv(max_depth=2, predictor=ShortestPathPredictorForRailEnv())\n",
    "LocalGridObs = LocalObsForRailEnv(view_height=10, view_width=2, center=2)\n",
    "rail_generator = complex_rail_generator(nr_start_goal=10, nr_extra=2, min_dist=8, max_dist=99999, seed=1)\n",
    "env = RailEnv(width=20, height=20,\n",
    "              rail_generator= rail_generator,\n",
    "              schedule_generator=complex_schedule_generator(), \n",
    "              number_of_agents=N_agents, \n",
    "              obs_builder_object=TreeObservation)\n",
    "env.reset()\n",
    "\n",
    "env_renderer = RenderTool(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your own Agent or use RLlib to train agents on Flatland\n",
    "# As an example we use a random agent here\n",
    "class RandomAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        :param state: input is the observation of the agent\n",
    "        :return: returns an action\n",
    "        \"\"\"\n",
    "        return np.random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def step(self, memories):\n",
    "        \"\"\"\n",
    "        Step function to improve agent by adjusting policy given the observations\n",
    "\n",
    "        :param memories: SARS Tuple to be\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    def save(self, filename):\n",
    "        # Store the current policy\n",
    "        return\n",
    "\n",
    "    def load(self, filename):\n",
    "        # Load a policy\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the agent with the parameters corresponding to the environment and observation_builder\n",
    "agent = RandomAgent(218, 5)\n",
    "n_trials = 20\n",
    "\n",
    "# Empty dictionary for all agent action\n",
    "action_dict = dict()\n",
    "print(\"Starting Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open_window - pyglet\n",
      "Episode Nr. 1\t Score = -500.0\n",
      "Episode Nr. 2\t Score = -500.0\n",
      "Episode Nr. 3\t Score = -91.0\n",
      "Episode Nr. 4\t Score = -500.0\n",
      "Episode Nr. 5\t Score = -500.0\n",
      "Episode Nr. 6\t Score = -500.0\n",
      "Episode Nr. 7\t Score = -500.0\n",
      "Episode Nr. 8\t Score = -500.0\n",
      "Episode Nr. 9\t Score = -17.0\n",
      "Episode Nr. 10\t Score = -50.0\n",
      "Episode Nr. 11\t Score = -500.0\n",
      "Episode Nr. 12\t Score = -94.0\n",
      "Episode Nr. 13\t Score = -422.0\n",
      "Episode Nr. 14\t Score = -214.0\n",
      "Episode Nr. 15\t Score = -500.0\n",
      "Episode Nr. 16\t Score = -500.0\n",
      "Episode Nr. 17\t Score = -69.0\n",
      "Episode Nr. 18\t Score = -92.0\n",
      "Episode Nr. 19\t Score = -80.0\n",
      "Episode Nr. 20\t Score = -17.0\n"
     ]
    }
   ],
   "source": [
    "all_scores = []\n",
    "for trials in range(1, n_trials + 1):\n",
    "\n",
    "    # Reset environment and get initial observations for all agents\n",
    "    obs, info = env.reset()\n",
    "    for idx in range(env.get_num_agents()):\n",
    "        tmp_agent = env.agents[idx]\n",
    "        tmp_agent.speed_data[\"speed\"] = 1 / (idx + 1)\n",
    "    env_renderer.reset()\n",
    "    # Here you can also further enhance the provided observation by means of normalization\n",
    "    # See training navigation example in the baseline repository\n",
    "\n",
    "    score = 0\n",
    "    # Run episode\n",
    "    for step in range(500):\n",
    "        # Chose an action for each agent in the environment\n",
    "        for a in range(env.get_num_agents()):\n",
    "            action = agent.act(obs[a])\n",
    "            action_dict.update({a: action})\n",
    "        # Environment step which returns the observations for all agents, their corresponding\n",
    "        # reward and whether their are done\n",
    "        next_obs, all_rewards, done, _ = env.step(action_dict)\n",
    "        # note that rendering significantly slows down the average run time\n",
    "        # consider commenting the rendering command for a faster run\n",
    "        env_renderer.render_env(show=True, show_observations=True, show_predictions=False)\n",
    "\n",
    "        # Update replay buffer and train agent\n",
    "        for a in range(env.get_num_agents()):\n",
    "            agent.step((obs[a], action_dict[a], all_rewards[a], next_obs[a], done[a]))\n",
    "            score += all_rewards[a]\n",
    "        obs = next_obs.copy()\n",
    "        if done['__all__']:\n",
    "            break\n",
    "    all_scores.append(score)        \n",
    "    print('Episode Nr. {}\\t Score = {}'.format(trials, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score of a random agent over 20 episodes is -307.3\n"
     ]
    }
   ],
   "source": [
    "avg = np.mean(all_scores)\n",
    "print('average score of a random agent over {} episodes is {}'.format(n_trials, avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flatland-rl]",
   "language": "python",
   "name": "conda-env-flatland-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
