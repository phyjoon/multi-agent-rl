{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent \n",
    "\n",
    "This is a random agent as implemented in examples provided [here](https://gitlab.aicrowd.com/flatland/flatland/blob/master/examples/training_example.py) by the Flatland challenge creators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from flatland.envs.observations import TreeObsForRailEnv, LocalObsForRailEnv\n",
    "from flatland.envs.predictions import ShortestPathPredictorForRailEnv\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.envs.rail_generators import complex_rail_generator\n",
    "from flatland.envs.schedule_generators import complex_schedule_generator\n",
    "from flatland.utils.rendertools import RenderTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Use the complex_rail_generator to generate feasible network configurations with corresponding tasks\n",
    "# Training on simple small tasks is the best way to get familiar with the environment\n",
    "N_agents = 1\n",
    "\n",
    "TreeObservation = TreeObsForRailEnv(max_depth=2, predictor=ShortestPathPredictorForRailEnv())\n",
    "LocalGridObs = LocalObsForRailEnv(view_height=10, view_width=2, center=2)\n",
    "rail_generator = complex_rail_generator(nr_start_goal=10, nr_extra=2, min_dist=8, max_dist=99999, seed=1)\n",
    "env = RailEnv(width=20, height=20,\n",
    "              rail_generator= rail_generator,\n",
    "              schedule_generator=complex_schedule_generator(), \n",
    "              number_of_agents=N_agents, \n",
    "              obs_builder_object=TreeObservation)\n",
    "env.reset()\n",
    "\n",
    "env_renderer = RenderTool(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your own Agent or use RLlib to train agents on Flatland\n",
    "# As an example we use a random agent here\n",
    "class RandomAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        :param state: input is the observation of the agent\n",
    "        :return: returns an action\n",
    "        \"\"\"\n",
    "        return np.random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def step(self, memories):\n",
    "        \"\"\"\n",
    "        Step function to improve agent by adjusting policy given the observations\n",
    "\n",
    "        :param memories: SARS Tuple to be\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    def save(self, filename):\n",
    "        # Store the current policy\n",
    "        return\n",
    "\n",
    "    def load(self, filename):\n",
    "        # Load a policy\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the agent with the parameters corresponding to the environment and observation_builder\n",
    "agent = RandomAgent(218, 5)\n",
    "n_trials = 20\n",
    "\n",
    "# Empty dictionary for all agent action\n",
    "action_dict = dict()\n",
    "print(\"Starting Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open_window - pyglet\n"
     ]
    }
   ],
   "source": [
    "all_scores = []\n",
    "for trials in range(1, n_trials + 1):\n",
    "\n",
    "    # Reset environment and get initial observations for all agents\n",
    "    obs, info = env.reset()\n",
    "    for idx in range(env.get_num_agents()):\n",
    "        tmp_agent = env.agents[idx]\n",
    "        tmp_agent.speed_data[\"speed\"] = 1 / (idx + 1)\n",
    "    env_renderer.reset()\n",
    "    # Here you can also further enhance the provided observation by means of normalization\n",
    "    # See training navigation example in the baseline repository\n",
    "\n",
    "    score = 0\n",
    "    # Run episode\n",
    "    for step in range(50):\n",
    "        # Chose an action for each agent in the environment\n",
    "        for a in range(env.get_num_agents()):\n",
    "            action = agent.act(obs[a])\n",
    "            action_dict.update({a: action})\n",
    "        # Environment step which returns the observations for all agents, their corresponding\n",
    "        # reward and whether their are done\n",
    "        next_obs, all_rewards, done, _ = env.step(action_dict)\n",
    "        # note that rendering significantly slows down the average run time\n",
    "        # consider commenting the rendering command for a faster run\n",
    "        env_renderer.render_env(show=True, show_observations=True, show_predictions=False)\n",
    "\n",
    "        # Update replay buffer and train agent\n",
    "        for a in range(env.get_num_agents()):\n",
    "            agent.step((obs[a], action_dict[a], all_rewards[a], next_obs[a], done[a]))\n",
    "            score += all_rewards[a]\n",
    "        obs = next_obs.copy()\n",
    "        if done['__all__']:\n",
    "            break\n",
    "    all_scores.append(score)        \n",
    "    print('Episode Nr. {}\\t Score = {}'.format(trials, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = np.mean(all_scores)\n",
    "print('average score of a random agent over {} episodes is {}'.format(n_trials, avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent with renderer without background embellishments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to check that the renderer works fine even after removing background embellishments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "#import tkinter as tk\n",
    "\n",
    "from flatland.envs.observations import TreeObsForRailEnv, LocalObsForRailEnv\n",
    "from flatland.envs.predictions import ShortestPathPredictorForRailEnv\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.envs.rail_generators import complex_rail_generator\n",
    "from flatland.envs.schedule_generators import complex_schedule_generator\n",
    "from flatland.utils.rendertools import RenderTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from numpy import array\n",
    "from pkg_resources import resource_string as resource_bytes\n",
    "\n",
    "from flatland.utils.graphics_layer import GraphicsLayer\n",
    "\n",
    "from flatland.core.grid.rail_env_grid import RailEnvTransitions \n",
    "\n",
    "from flatland.utils.graphics_pil import PILGL, PILSVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Use the complex_rail_generator to generate feasible network configurations with corresponding tasks\n",
    "# Training on simple small tasks is the best way to get familiar with the environment\n",
    "N_agents = 1\n",
    "\n",
    "TreeObservation = TreeObsForRailEnv(max_depth=2, predictor=ShortestPathPredictorForRailEnv())\n",
    "LocalGridObs = LocalObsForRailEnv(view_height=10, view_width=2, center=2)\n",
    "rail_generator = complex_rail_generator(nr_start_goal=10, nr_extra=2, min_dist=8, max_dist=99999, seed=1)\n",
    "env = RailEnv(width=20, height=20,\n",
    "              rail_generator= rail_generator,\n",
    "              schedule_generator=complex_schedule_generator(), \n",
    "              number_of_agents=N_agents, \n",
    "              obs_builder_object=TreeObservation)\n",
    "env.reset()\n",
    "\n",
    "env_renderer = RenderTool(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_rail_at_new(self, row, col, binary_trans, target=None, is_selected=False, rail_grid=None, num_agents=None,\n",
    "                show_debug=True):\n",
    "    \n",
    "    if binary_trans in self.pil_rail:\n",
    "        pil_track = self.pil_rail[binary_trans]\n",
    "        if target is not None:\n",
    "            target_img = self.station_colors[target % len(self.station_colors)]\n",
    "            target_img = Image.alpha_composite(pil_track, target_img)\n",
    "            self.draw_image_row_col(target_img, (row, col), layer=PILGL.TARGET_LAYER)\n",
    "            if show_debug:\n",
    "                self.text_rowcol((row + 0.8, col + 0.0), strText=str(target), layer=PILGL.TARGET_LAYER)\n",
    "\n",
    "        city_size = 1\n",
    "        if num_agents is not None:\n",
    "            city_size = max(1, np.log(1 + num_agents) / 2.5)\n",
    "\n",
    "        self.draw_image_row_col(pil_track, (row, col), layer=PILGL.RAIL_LAYER)\n",
    "    else:\n",
    "        print(\"Illegal rail:\", row, col, format(binary_trans, \"#018b\")[2:], binary_trans)\n",
    "\n",
    "    if target is not None:\n",
    "        if is_selected:\n",
    "            svgBG = self.pil_from_png_file('flatland.png', \"Selected_Target.png\")\n",
    "            self.clear_layer(PILGL.SELECTED_TARGET_LAYER, 0)\n",
    "            self.draw_image_row_col(svgBG, (row, col), layer=PILGL.SELECTED_TARGET_LAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcType = types.MethodType\n",
    "env_renderer = RenderTool(env)\n",
    "env_renderer.gl.set_rail_at = funcType(set_rail_at_new, env_renderer.gl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your own Agent or use RLlib to train agents on Flatland\n",
    "# As an example we use a random agent here\n",
    "class RandomAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        :param state: input is the observation of the agent\n",
    "        :return: returns an action\n",
    "        \"\"\"\n",
    "        return np.random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def step(self, memories):\n",
    "        \"\"\"\n",
    "        Step function to improve agent by adjusting policy given the observations\n",
    "\n",
    "        :param memories: SARS Tuple to be\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    def save(self, filename):\n",
    "        # Store the current policy\n",
    "        return\n",
    "\n",
    "    def load(self, filename):\n",
    "        # Load a policy\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the agent with the parameters corresponding to the environment and observation_builder\n",
    "agent = RandomAgent(218, 5)\n",
    "n_trials = 20\n",
    "\n",
    "# Empty dictionary for all agent action\n",
    "action_dict = dict()\n",
    "print(\"Starting Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open_window - pyglet\n",
      "Episode Nr. 1\t Score = -50.0\n",
      "Episode Nr. 2\t Score = -50.0\n",
      "Episode Nr. 3\t Score = -24.0\n",
      "Episode Nr. 4\t Score = -50.0\n",
      "Episode Nr. 5\t Score = -50.0\n",
      "Episode Nr. 6\t Score = -50.0\n",
      "Episode Nr. 7\t Score = -50.0\n",
      "Episode Nr. 8\t Score = -50.0\n",
      "Episode Nr. 9\t Score = -50.0\n",
      "Episode Nr. 10\t Score = -50.0\n",
      "Episode Nr. 11\t Score = -50.0\n",
      "Episode Nr. 12\t Score = -50.0\n",
      "Episode Nr. 13\t Score = -50.0\n",
      "Episode Nr. 14\t Score = -50.0\n",
      "Episode Nr. 15\t Score = -14.0\n",
      "Episode Nr. 16\t Score = -50.0\n",
      "Episode Nr. 17\t Score = -25.0\n",
      "Episode Nr. 18\t Score = -50.0\n",
      "Episode Nr. 19\t Score = -26.0\n",
      "Episode Nr. 20\t Score = -33.0\n"
     ]
    }
   ],
   "source": [
    "all_scores = []\n",
    "for trials in range(1, n_trials + 1):\n",
    "\n",
    "    # Reset environment and get initial observations for all agents\n",
    "    obs, info = env.reset()\n",
    "    for idx in range(env.get_num_agents()):\n",
    "        tmp_agent = env.agents[idx]\n",
    "        tmp_agent.speed_data[\"speed\"] = 1 / (idx + 1)\n",
    "    env_renderer.reset()\n",
    "    # Here you can also further enhance the provided observation by means of normalization\n",
    "    # See training navigation example in the baseline repository\n",
    "\n",
    "    score = 0\n",
    "    # Run episode\n",
    "    for step in range(50):\n",
    "        # Chose an action for each agent in the environment\n",
    "        for a in range(env.get_num_agents()):\n",
    "            action = agent.act(obs[a])\n",
    "            action_dict.update({a: action})\n",
    "        # Environment step which returns the observations for all agents, their corresponding\n",
    "        # reward and whether their are done\n",
    "        next_obs, all_rewards, done, _ = env.step(action_dict)\n",
    "        # note that rendering significantly slows down the average run time\n",
    "        # consider commenting the rendering command for a faster run\n",
    "        env_renderer.render_env(show=True, show_observations=True, show_predictions=False)\n",
    "\n",
    "        # Update replay buffer and train agent\n",
    "        for a in range(env.get_num_agents()):\n",
    "            agent.step((obs[a], action_dict[a], all_rewards[a], next_obs[a], done[a]))\n",
    "            score += all_rewards[a]\n",
    "        obs = next_obs.copy()\n",
    "        if done['__all__']:\n",
    "            break\n",
    "    all_scores.append(score)        \n",
    "    print('Episode Nr. {}\\t Score = {}'.format(trials, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score of a random agent over 20 episodes is -43.6\n"
     ]
    }
   ],
   "source": [
    "avg = np.mean(all_scores)\n",
    "print('average score of a random agent over {} episodes is {}'.format(n_trials, avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flatland-rl]",
   "language": "python",
   "name": "conda-env-flatland-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
